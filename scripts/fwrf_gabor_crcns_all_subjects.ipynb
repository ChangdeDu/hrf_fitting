{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use feature-weighted rf model on the crcns vim-1 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import pickle\n",
    "from time import time\n",
    "from glob import glob\n",
    "from scipy.io import loadmat\n",
    "from PIL import Image\n",
    "from os.path import join\n",
    "from scipy.stats import pearsonr\n",
    "from hrf_fitting.src.feature_weighted_rf_models import make_rf_table,receptive_fields, model_space, prediction_menu, bigmult\n",
    "from hrf_fitting.src.feature_weighted_rf_models import train_fwrf_model\n",
    "from hrf_fitting.src.gabor_feature_dictionaries import gabor_feature_maps\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "version_number = '1p1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Load crcns stimuli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load crcns stimuli "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##known stimulus parameters\n",
    "Ttrn = 1750\n",
    "Tval = 120\n",
    "S = 500\n",
    "T = Ttrn+Tval\n",
    "train_stim_files = glob('/media/tnaselar/Data/crcns_datasets/vim-1/Stimuli_Trn_FullRes*.mat')\n",
    "val_stim_file = '/media/tnaselar/Data/crcns_datasets/vim-1/Stimuli_Val_FullRes.mat'\n",
    "n_image_channels = 1 ##could be 3 for color images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##allocate memory for stim\n",
    "training_stim = np.zeros((Ttrn,S,S),dtype='float32')\n",
    "\n",
    "##load training stim\n",
    "cnt = 0\n",
    "for sl in sorted(train_stim_files):\n",
    "    this_h5 = h5py.File(sl,'r')\n",
    "    this_train_stim = this_h5['stimTrn']\n",
    "    this_num_stim = this_train_stim.shape[-1]\n",
    "    training_stim[cnt:cnt+this_num_stim,:,:] = np.transpose(this_train_stim[:],[2,1,0])\n",
    "    cnt += this_num_stim\n",
    "    this_h5.close()\n",
    "    \n",
    "##load validation stim\n",
    "val_h5 = h5py.File(val_stim_file,'r')\n",
    "validation_stim = np.transpose(val_h5['stimVal'][:],[2,1,0])\n",
    "val_h5.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(validation_stim[0,:,:],cmap='gray')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(training_stim[-1,:,:],cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: construct feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_orientations = 8\n",
    "deg_per_stimulus = 20\n",
    "lowest_sp_freq = .25 ##cyc/deg\n",
    "highest_sp_freq = 6.25\n",
    "num_sp_freq = 8\n",
    "pix_per_cycle = 4#2.13333333\n",
    "complex_cell = True\n",
    "\n",
    "print 'D = total number of features = %d' %(n_orientations * num_sp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### construct gabor wavelet stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gfm = gabor_feature_maps(n_orientations,\n",
    "                         deg_per_stimulus,\n",
    "                         (lowest_sp_freq,highest_sp_freq,num_sp_freq),\n",
    "                         pix_per_cycle=pix_per_cycle,complex_cell=complex_cell,\n",
    "                         diams_per_filter = 4,\n",
    "                        cycles_per_radius = 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gfm.gbr_table.head(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gfm.filter_stack.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### see one of the gabors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "o =  8##choose an orientation\n",
    "plt.imshow(np.imag(gfm.filter_stack[o,0,:,:]),cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: receptive fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deg_per_radius = (.25, 8, 8) ##rf sizes in degrees (smallest, largest, number of sizes)\n",
    "spacing = .75 ##spacing between rf's in degrees\n",
    "rf = receptive_fields(deg_per_stimulus,deg_per_radius,spacing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf.rf_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'G = number of rf models = %d' %(rf.rf_table.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Model space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### apply an activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_act_func(x):\n",
    "    return np.log(1+np.sqrt(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### instantiate model space object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##construct the model space\n",
    "init_feat_dict = gfm.create_feature_maps(training_stim[0,np.newaxis,np.newaxis,:,:])\n",
    "ms = model_space(init_feat_dict, rf, activation_function = log_act_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### construct training/validation model space tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##loop over training stimuli because feature maps for all training stim. > 48Gb\n",
    "training_mst = np.zeros((ms.receptive_fields.G, Ttrn, ms.D)).astype('float32')\n",
    "\n",
    "num_chunks = 2\n",
    "stim_dx = np.linspace(0,T-1,num=num_chunks+1, endpoint=True,dtype='int')\n",
    "\n",
    "cnt = 0\n",
    "for t in stim_dx[1:]:\n",
    "    this_training_feature_dict = gfm.create_feature_maps(training_stim[cnt:cnt+t,np.newaxis,:,:])\n",
    "    training_mst[:,cnt:cnt+t,:] = ms.construct_model_space_tensor(this_training_feature_dict,normalize=False)\n",
    "    cnt += t\n",
    "\n",
    "##clear up memory\n",
    "this_training_feature_dict = []\n",
    "\n",
    "##normalize and save normalization constants\n",
    "training_mst = ms.normalize_model_space_tensor(training_mst, save=True)\n",
    "\n",
    "\n",
    "##should work in one shot for because not too big\n",
    "val_feature_dict = gfm.create_feature_maps(validation_stim[:,np.newaxis,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "validation_mst = ms.construct_model_space_tensor(val_feature_dict,normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: load data and fit models in a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_stuff(save_to_this_file, data_objects_dict):\n",
    "    failed = []\n",
    "    with h5py.File(save_to_this_file+'.h5py', 'w') as hf:\n",
    "        for k,v in data_objects_dict.iteritems():\n",
    "            try:\n",
    "                hf.create_dataset(k,data=v)\n",
    "                print 'saved %s in h5py file' %(k)\n",
    "            except:\n",
    "                failed.append(k)\n",
    "                print 'failed to save %s as h5py. will try pickle' %(k)\n",
    "    \n",
    "    for k in failed:\n",
    "        with open(save_to_this_file+'_'+'%s.pkl' %(k), 'w') as pkl:\n",
    "            try:\n",
    "                pickle.dump(data_objects_dict[k],pkl)\n",
    "                print 'saved %s as pkl' %(k)\n",
    "            except:\n",
    "                print 'failed to save %s in any format. lost.' %(k)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subjects = ['S1','S2']\n",
    "saving_dir = '/media/tnaselar/Data/fwrf/Gabor_model/'\n",
    "voxel_file = '/media/tnaselar/Data/crcns_datasets/vim-1/EstimatedResponses.mat'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with  h5py.File(voxel_file,'r') as crcns_voxel_data:\n",
    "    for subj in subjects:\n",
    "        saving_file = join(saving_dir, 'gabor_model_%s' %(subj))\n",
    "        save_this_data = {}\n",
    "        save_this_data['model_space'] = ms\n",
    "        #remove nans, becuase this data-set has some. otherwise even one nan will infect gradient for every voxel.\n",
    "        voxel_data = np.concatenate((crcns_voxel_data['dataVal'+subj],crcns_voxel_data['dataTrn'+subj])\n",
    "                                    ,axis=0).astype('float32')\n",
    "        no_nan = np.isnan(voxel_data).sum(axis=0) == 0 ##<<pull voxels with nans \n",
    "        voxel_data = voxel_data[:,no_nan]\n",
    "        print voxel_data.shape\n",
    "        V = voxel_data.shape[1]\n",
    "        #get training/validation views on voxel_data\n",
    "        nvox=V\n",
    "        trnIdx = np.arange(Tval,T)\n",
    "        valIdx = np.arange(0,Tval)\n",
    "        trn_voxel_data = voxel_data[trnIdx,0:nvox]\n",
    "        val_voxel_data = voxel_data[valIdx,0:nvox]\n",
    "        #train the model!\n",
    "        fvl,ffw,frf,beh = train_fwrf_model(training_mst,\n",
    "                                           trn_voxel_data,\n",
    "                                           initial_feature_weights='zeros',\n",
    "                                           voxel_binsize = nvox,\n",
    "                                           rf_grid_binsize=10,\n",
    "                                           learning_rate=10**(-5.3),\n",
    "                                           max_iters = 200,\n",
    "                                           early_stop_fraction=0.2,\n",
    "                                           report_every = 100)\n",
    "                                           \n",
    "                                          \n",
    "\n",
    "        save_this_data['fvl'] = fvl\n",
    "        save_this_data['ffw'] = ffw\n",
    "        save_this_data['frf'] = frf\n",
    "        save_this_data['beh'] = beh\n",
    "\n",
    "        ##generate predictions one voxel at a time\n",
    "        pred = np.zeros((Tval,nvox))\n",
    "        for v in range(nvox):  ##<<some kind of bug in training function, last voxel getting skipped.\n",
    "            pred[:,v] = np.squeeze(bigmult(validation_mst[np.newaxis,frf[v],:,:],\n",
    "                                           ffw[np.newaxis,:,v, np.newaxis]))\n",
    "        save_this_data['pred'] = pred\n",
    "\n",
    "        ##get correlation = prediction accuracy\n",
    "        val_cc = np.zeros(nvox)  \n",
    "        for v in range(nvox): \n",
    "            val_cc[v] = pearsonr(val_voxel_data[:,v],pred[:,v])[0]\n",
    "        val_cc = np.nan_to_num(val_cc)\n",
    "        save_this_data['val_cc'] = val_cc\n",
    "\n",
    "        save_stuff(saving_file, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##close data file\n",
    "crcns_voxel_data.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
