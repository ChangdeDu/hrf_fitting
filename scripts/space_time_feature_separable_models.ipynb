{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature map models with time-space-feature-seperable weighting functions\n",
    "Here we're going to code up models with the following form:\n",
    "\n",
    "$$r_t = \\sum_{i}\\sum_{\\tau}\\sum_{x,y} \\phi_{i}(s(t-\\tau,x,y))w(\\tau,x,y; \\theta))\\nu_{i}+\\epsilon$$\n",
    "\n",
    "Where $r_t$ is the response of some neural unit at time $t$, $(x,y)$ are spatial coordinates, and $\\phi_i$ is a nonlinear feature map that is a function of a movie $s(t-\\tau,x,y)$. The adjustable parameters are $\\theta$, which specifies the space-time kernel $w$, and a set of independent features weights $\\nu_{i}$.\n",
    "\n",
    "In this model, we assume that the neuron views each feature map through the same spatial and temporal window $w$, but that it weights each feature map $\\phi_i$ according to it's importance $\\nu_i$. This is the fundamental assumption and it seems to be pretty valid, given what I know about vision at least.\n",
    "\n",
    "Our goal will be to infer the space-time kernel and feature weights that best predict measured activity $r$. We can perform this optimization for many voxels/neurons. The feature maps will be the same for each voxel/neuron, but the space-time kernel and feature weights will change according to the characteristics of each.\n",
    "\n",
    "In this version of the model feature and space-time selectivity are separable. Also, the space-time kernel is written as a parameterized function, while the feature weights are completely flexible--i.e., there is one independent feature weight per feature map. We can consider other variants, for example, where space and time are also separable, and where the feature weights are also a parameterized function of the features:\n",
    "\n",
    "$$r_t = \\sum_{i}\\sum_{\\tau}\\sum_{x,y} \\phi_{i}(s(t-\\tau,x,y))g(x,y; \\theta_{space}))f(\\tau; \\theta_{time}) \\nu(i; \\theta_{feature})+\\epsilon$$ \n",
    "\n",
    "where $g$, $f$, and $\\nu$ are functions of space, time, and feature, respectively.\n",
    "\n",
    "An interesting variant would add an activation nonlinearity before application of the feature weights:\n",
    "\n",
    "$$r_t = \\sum_{i}\\sum_{\\tau}\\sum_{x,y} act(\\phi_{i}(s(t-\\tau,x,y))g(x,y; \\theta_{space}))f(\\tau; \\theta_{time}); exponent) \\nu(i; \\theta_{feature})+\\epsilon$$ \n",
    "\n",
    "The more we separate, the more interpretable the model becomes, and the fewer parameters we have to fit. The crucial separation is space/feature, so that the number of parameters = *number of pixels* + *number of features*, instead of the product of these two terms. The more we separate, of course, the more likely we are to miss potentially important interactions between space, time, and feature.\n",
    "\n",
    "Currently the code is being designed to accomodate two kinds of optimiziation: a brute force grid search (the G dimension below) and some kind of regression or gradient-based procedure. The idea is that there are only a few RF kernel parameters so we can probably optimize these via a stupid brute-force search. Meanwhile, there are likely to be many feature map weights, so we optimize these using regression or gradient methods. The potentially nice thing about brute-forcing the RF params is that we won't have to do an iterative optimization procedure (in principle). Instead we regress over all possible RFkernels, and take the best one. It's an empirical question what params are best brute-forced, and which are best folded into the gradient descent procedure. The goal, however, is to not resort to an iterative, EM-like scheme where we have to assume values of one set of parameters in order to optimize the other set, and so forth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 760\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from theano import tensor as tnsr\n",
    "from theano import function, scan\n",
    "from time import time\n",
    "\n",
    "\n",
    "from hrf_fitting.src.features import make_complex_gabor as gaborme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some code for making feature maps.\n",
    "For example, with Gabors.\n",
    "This stage produces the feature maps =  T x D tensor. \n",
    "\n",
    "Question: normalize w.r.t. feature maps, or w.r.t. pixels within feature maps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cycles per deg.</th>\n",
       "      <th>cycles per pixel</th>\n",
       "      <th>cycles per stimulus</th>\n",
       "      <th>fwhm (pix)</th>\n",
       "      <th>n_pix</th>\n",
       "      <th>pixels per cycle</th>\n",
       "      <th>prf_size (deg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.010008</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>0.221184</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>3472.222222</td>\n",
       "      <td>11.050042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.331119</td>\n",
       "      <td>0.009528</td>\n",
       "      <td>7.317760</td>\n",
       "      <td>104.950149</td>\n",
       "      <td>262.375372</td>\n",
       "      <td>104.950149</td>\n",
       "      <td>1.510031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.652230</td>\n",
       "      <td>0.018769</td>\n",
       "      <td>14.414336</td>\n",
       "      <td>53.280290</td>\n",
       "      <td>133.200725</td>\n",
       "      <td>53.280290</td>\n",
       "      <td>0.766601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.973341</td>\n",
       "      <td>0.028009</td>\n",
       "      <td>21.510912</td>\n",
       "      <td>35.702810</td>\n",
       "      <td>89.257025</td>\n",
       "      <td>35.702810</td>\n",
       "      <td>0.513695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.294452</td>\n",
       "      <td>0.037249</td>\n",
       "      <td>28.607488</td>\n",
       "      <td>26.846118</td>\n",
       "      <td>67.115295</td>\n",
       "      <td>26.846118</td>\n",
       "      <td>0.386264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.615562</td>\n",
       "      <td>0.046490</td>\n",
       "      <td>35.704064</td>\n",
       "      <td>21.510156</td>\n",
       "      <td>53.775391</td>\n",
       "      <td>21.510156</td>\n",
       "      <td>0.309490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.936673</td>\n",
       "      <td>0.055730</td>\n",
       "      <td>42.800640</td>\n",
       "      <td>17.943657</td>\n",
       "      <td>44.859142</td>\n",
       "      <td>17.943657</td>\n",
       "      <td>0.258175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.257784</td>\n",
       "      <td>0.064970</td>\n",
       "      <td>49.897216</td>\n",
       "      <td>15.391640</td>\n",
       "      <td>38.479101</td>\n",
       "      <td>15.391640</td>\n",
       "      <td>0.221456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.578895</td>\n",
       "      <td>0.074211</td>\n",
       "      <td>56.993792</td>\n",
       "      <td>13.475152</td>\n",
       "      <td>33.687880</td>\n",
       "      <td>13.475152</td>\n",
       "      <td>0.193881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.900006</td>\n",
       "      <td>0.083451</td>\n",
       "      <td>64.090368</td>\n",
       "      <td>11.983080</td>\n",
       "      <td>29.957700</td>\n",
       "      <td>11.983080</td>\n",
       "      <td>0.172413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cycles per deg.  cycles per pixel  cycles per stimulus  fwhm (pix)  \\\n",
       "0         0.010008          0.000288             0.221184  768.000000   \n",
       "1         0.331119          0.009528             7.317760  104.950149   \n",
       "2         0.652230          0.018769            14.414336   53.280290   \n",
       "3         0.973341          0.028009            21.510912   35.702810   \n",
       "4         1.294452          0.037249            28.607488   26.846118   \n",
       "5         1.615562          0.046490            35.704064   21.510156   \n",
       "6         1.936673          0.055730            42.800640   17.943657   \n",
       "7         2.257784          0.064970            49.897216   15.391640   \n",
       "8         2.578895          0.074211            56.993792   13.475152   \n",
       "9         2.900006          0.083451            64.090368   11.983080   \n",
       "\n",
       "        n_pix  pixels per cycle  prf_size (deg)  \n",
       "0  768.000000       3472.222222       11.050042  \n",
       "1  262.375372        104.950149        1.510031  \n",
       "2  133.200725         53.280290        0.766601  \n",
       "3   89.257025         35.702810        0.513695  \n",
       "4   67.115295         26.846118        0.386264  \n",
       "5   53.775391         21.510156        0.309490  \n",
       "6   44.859142         17.943657        0.258175  \n",
       "7   38.479101         15.391640        0.221456  \n",
       "8   33.687880         13.475152        0.193881  \n",
       "9   29.957700         11.983080        0.172413  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##parameters for designing gabor feature maps \n",
    "pixel_per_deg = 34.751    ##determined by experiment 34.751 = match/nonmatch exp.\n",
    "pixels_per_stimulus = 768 ##det. by exp. 768 = match/nonmatch exp.\n",
    "cycles_per_pixel = np.linspace(0.000288,0.083451,num=10)  ##cyc/pix\n",
    "cycles_per_fwhm = 1.0\n",
    "fwhms_per_kernel = 2.5 ##determines how big the gabor should be.\n",
    "\n",
    "metrics = {'cycles per pixel':cycles_per_pixel,\n",
    "           'pixels per cycle': 1./cycles_per_pixel,\n",
    "           'cycles per stimulus': cycles_per_pixel*pixels_per_stimulus,\n",
    "           'cycles per deg.': cycles_per_pixel*pixel_per_deg,\n",
    "           'fwhm (pix)': np.clip(cycles_per_fwhm/cycles_per_pixel,0,pixels_per_stimulus),\n",
    "           'prf_size (deg)': np.clip(cycles_per_fwhm/cycles_per_pixel,0,pixels_per_stimulus)/pixel_per_deg/2.,\n",
    "           'n_pix': np.clip(fwhms_per_kernel*cycles_per_fwhm/cycles_per_pixel,0,pixels_per_stimulus)}\n",
    "\n",
    "fm = pd.DataFrame(metrics)\n",
    "fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx = 9\n",
    "ori = np.pi/2.\n",
    "center = (0,0)\n",
    "freq = fm.loc[idx,'cycles per stimulus']\n",
    "fwhm = fm.loc[idx,'fwhm (pix)']\n",
    "n_pix = int(fm.loc[idx,'n_pix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29, 29)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPwAAAD8CAYAAABTq8lnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n37DhjPD/AVzawY4rkf/1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9ba6d32a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "foo = gaborme(freq,ori,center,fwhm,n_pix)\n",
    "plt.imshow(np.real(foo),cmap='gray')\n",
    "print foo.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply RF kernels in a grid\n",
    "Takes the feature maps (T x D tensor) and applies multiple RF kernels. \n",
    "Produces a G x T x D tensor that we refer to as the \"model space\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Space-time kernel function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def space_time_kernel(center, fwhm, n_pix, time_kernel):\n",
    "    '''\n",
    "    a separable space-time kernel.\n",
    "    space kernel is gaussian\n",
    "    time kernel is an array of length T\n",
    "    returns a 3-D volume that is (n_pix,n_pix,T)\n",
    "    slice it to see what it does.\n",
    "    '''\n",
    "    space_kernel = np.atleast_3d(make_gaussian(center,fwhm,n_pix))\n",
    "    return time_kernel*space_kernel\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### construct a brute-force grid over spatial and temporal kernel and activation function params."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of feature-weight tensor to model-space tensor in Theano\n",
    "Something like this is what we'd need to generate predictions before we've decided what\n",
    "the RF params are going to be. So this code generates predictions for all RFs, all voxels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Some fake dimensions for testing\n",
    "D = 1000  ##number of feature maps\n",
    "T = 1100  ##number of data samples\n",
    "G = 100   ##coarse grid of r.f. centers/positions\n",
    "V = 100   ##voxels (chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = tnsr.tensor3('X')    ##model-space tensor: G x T x D\n",
    "NU = tnsr.tensor3('NU')  ##feature weight tensor: G x D x V\n",
    "Z = tnsr.batched_tensordot(X,NU,axes=[[2],[1]]) ##prediction tensor: G x T x V\n",
    "bigmult = function([X,NU], Z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model_space = np.random.rand(G,T,D).astype('float32')    ##RFparams x Trials x Dimensions\n",
    "feature_weights = np.random.rand(G,D,V).astype('float32') ##RFparams x Dimensions x Voxels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1100, 100)\n",
      "9.11011695862\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "print np.einsum('gtd,gdv->gtv',model_space,feature_weights).shape  ##RFparams x Trials x Voxels\n",
    "print time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1100, 100)\n",
      "0.161598920822\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "print bigmult(model_space,feature_weights).shape\n",
    "print time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "64**2*16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "^^^ For basic operation (data x weights) theano can be about x100 faster, although this seems to depend upon what else you're doing with the computer at the time. Seems important to start \"fresh\", with nothing else in memory.\n",
    "\n",
    "For 400K voxels and grid size of (centers x widths x activation exponent) = (64^2 x 8 x 8) we're looking at at least 190hrs on the GPU. Probably will take more time because this doesn't include the feature-weight optimization.\n",
    "\n",
    "If we are going to do grad. desc. on the feature weights we could potentially fold some of the RF or activation function parameters into the gradient..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection over brute-force grid (G)\n",
    "Calculate error over grid, then argmax it to generate final model for each voxel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### some fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "voxel_data = np.random.rand(T,V).astype('float32')\n",
    "prediction_menu = np.random.rand(G,T,V).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the error function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = tnsr.matrix('y')  ##voxel data tensor: T x V\n",
    "\n",
    "diff = y-Z  ##difference tensor: (T x V) - (G x T x V) = (G x T x V)\n",
    "sq_diff = (diff*diff).sum(axis=1) ##sum-sqaured-diffs tensor: G x V\n",
    "sq_diff_func = function([y,Z],sq_diff)  ##what will this do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100)\n"
     ]
    }
   ],
   "source": [
    "error_menu = sq_diff_func(voxel_data,prediction_menu)\n",
    "print error_menu.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_idx = error_menu.argmax(axis=0)\n",
    "best_model_idx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Gradient w.r.t. feature weights for many voxels choices of rf params (at once).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define a gradient\n",
    "Theano only takes gradients of functions with scalar outputs. Here we have G x V outputs of the cost function and we want to take gradient with respect to the weights that are specific to each (rf_param,voxel). Instead of doing a scan over all such pairs, we exploit fact that gradient w.r.t. weights not used to compute error for each (rf_param,voxel) are just zero. So we can take gradient of the SUM of the cost matrix w.r.t. entire matrix of feature weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SQD_sum = sq_diff.sum()  ##<<this is critical\n",
    "grad_SQD_wrt_NU = tnsr.grad(SQD_sum,NU) ##<<the summing trick above makes this easy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compute_grad = function(inputs = [y,X,NU], outputs=grad_SQD_wrt_NU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### clock and check against explicit gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1000, 100)\n",
      "0.19813990593\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "grrrr = compute_grad(voxel_data,model_space,feature_weights)\n",
    "print grrrr.shape\n",
    "print time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.328922272\n"
     ]
    }
   ],
   "source": [
    "##check for one voxel/rf combo\n",
    "guh = np.zeros(model_space[0,0,:].shape)\n",
    "prediction_menu = bigmult(model_space,feature_weights)\n",
    "start = time()\n",
    "\n",
    "##compute one gradient\n",
    "for t in range(T):\n",
    "    guh += 2*(voxel_data[t,0]-prediction_menu[0,t,0])*model_space[0,t,:]\n",
    "\n",
    "##best possible timing\n",
    "print (time()-start)*G*V  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  6.61754764,   6.1432045 ,   7.3624125 ,   6.38249694,\n",
       "         7.5335565 ,   6.1475923 ,   6.8695788 ,   6.22929511,\n",
       "         6.14372321,   6.855668  ,   6.35062476,   7.36832682,\n",
       "         6.54832894,   6.66065516,   6.55350642,   6.26397715,\n",
       "         6.68474655,   6.05754205,   6.36530005,   6.76825853,\n",
       "         6.17747046,   7.63193588,   7.32845471,   6.27968951,\n",
       "         6.45165978,   6.21345081,   6.23943307,   6.37073244,\n",
       "         6.09105955,   6.07782233,   7.26795183,   6.33278768,\n",
       "         6.45475608,   6.64677811,   6.25765285,   7.93617194,\n",
       "         6.51763807,   6.08901342,   6.60634824,   6.38702685,\n",
       "         7.02384474,   6.38460333,   6.60832025,   6.40166964,\n",
       "         6.36462104,   6.46272841,   6.14169673,   6.25457408,\n",
       "         6.69186247,   7.34425431,   6.4484126 ,   6.18534566,\n",
       "         6.09642416,   6.54604719,   7.43344591,   6.71044799,\n",
       "         7.86687706,   6.39815145,   6.31276534,   6.31328297,\n",
       "         6.73606178,   6.19305411,   6.89437807,   6.4780591 ,\n",
       "         6.3931043 ,   6.93426346,   7.12752678,   6.76533171,\n",
       "         6.25453462,   6.66247957,   6.72613318,   6.68812052,\n",
       "         7.25413306,   5.95641894,   6.41289509,   6.45570059,\n",
       "         6.76889732,   6.92161387,   6.75903137,   6.75977687,\n",
       "         6.30629453,   6.29054276,   6.18626285,   7.07662521,\n",
       "         7.11609585,   6.85581439,   6.65147754,   6.33917797,\n",
       "         6.11586295,   7.04249851,   6.40956237,   6.23416139,\n",
       "         6.41982811,   6.16208605,   6.60309835,   6.43389344,\n",
       "         7.53566247,   6.46885429,   6.32630479,   6.4520111 ,\n",
       "         6.28152853,   7.70830531,   6.52071932,   6.61100254,\n",
       "         6.48935356,   6.53499579,   6.73283003,   7.43797455,\n",
       "         6.47649155,   6.70263349,   6.31344626,   6.31707797,\n",
       "         6.7262145 ,   6.9264749 ,   6.54218187,   6.43251163,\n",
       "         6.22537724,   7.21911757,   6.72900716,   6.85210595,\n",
       "         7.12436588,   6.12202658,   6.90714688,   7.1675504 ,\n",
       "         6.14899863,   6.23545075,   6.66128124,   7.6599214 ,\n",
       "         7.097792  ,   6.34011089,   6.0548122 ,   6.55581523,\n",
       "         6.62640789,   6.32769437,   6.31628599,   6.40823012,\n",
       "         6.01059088,   6.53986908,   6.26249881,   6.49449373,\n",
       "         6.10668376,   6.27309602,   6.22497139,   6.55171369,\n",
       "         7.38366678,   6.94113097,   5.97435343,   6.7202807 ,\n",
       "         6.11771994,   6.51130678,   6.06462674,   6.35995023,\n",
       "         6.22274414,   6.48091679,   6.19408816,   6.56291274,\n",
       "         6.79130488,   6.10700673,   6.80655852,   6.36806486,\n",
       "         6.16958568,   6.28492595,   6.2760329 ,   7.12652173,\n",
       "         6.13009751,   6.55685652,   6.3508831 ,   6.18617007,\n",
       "         6.26582625,   6.40369518,   6.13190879,   6.20221336,\n",
       "         6.38433377,   6.28693739,   6.67469214,   7.87735767,\n",
       "         6.62733319,   6.27906976,   6.73891241,   6.75021133,\n",
       "         6.65542205,   6.47853497,   7.05022434,   6.15797348,\n",
       "         6.48162214,   6.96970064,   6.41086884,   6.22174674,\n",
       "         6.41114995,   6.12439574,   6.69988612,   6.60523716,\n",
       "         6.28190006,   6.48751635,   6.53628876,   6.40196638,\n",
       "         5.91557142,   6.47807161,   6.32936566,   6.44087281,\n",
       "         6.23016746,   6.79492282,   6.65802841,   6.23062743,\n",
       "         6.52983276,   7.38772577,   7.15592566,   6.55776741,\n",
       "         6.54825809,   6.20398827,   6.67293565,   6.67953865,\n",
       "         6.80769148,   6.47528967,   6.44093039,   6.44573821,\n",
       "         6.45403147,   6.5284276 ,   7.47952034,   6.41883444,\n",
       "         6.83623105,   6.90792941,   7.1643245 ,   6.35171181,\n",
       "         6.49633875,   7.24006936,   6.393284  ,   6.42110429,\n",
       "         7.23903469,   6.86669213,   6.28805944,   6.86164548,\n",
       "         6.36688137,   6.47105308,   6.87260934,   6.34168203,\n",
       "         6.4844412 ,   6.69136348,   6.40514969,   6.93647726,\n",
       "         7.37922735,   6.21914112,   6.16425325,   7.79352698,\n",
       "         7.7526511 ,   6.49755309,   6.42233217,   6.51822755,\n",
       "         6.65000242,   6.99093189,   6.36151512,   6.17997079,\n",
       "         6.37691449,   6.22453071,   7.29398643,   7.245369  ,\n",
       "         8.07653595,   6.54701903,   6.38499733,   6.19528865,\n",
       "         6.86129744,   7.05569965,   6.71050214,   6.94265663,\n",
       "         7.25380978,   6.12031502,   7.92996012,   6.70672457,\n",
       "         6.2912523 ,   6.05616428,   7.1414305 ,   6.45939928,\n",
       "         6.30656563,   6.12789053,   6.21682837,   6.99643246,\n",
       "         6.52775331,   7.05775174,   6.3595983 ,   7.01583859,\n",
       "         6.445451  ,   6.20698696,   6.96406086,   6.21696882,\n",
       "         6.81895279,   6.15244838,   6.88705927,   6.28250921,\n",
       "         6.27029806,   6.66756298,   6.18382538,   6.34739191,\n",
       "         6.83935197,   7.63986883,   7.86223846,   6.44782657,\n",
       "         6.96631403,   7.46338719,   6.268149  ,   6.33591821,\n",
       "         7.87507488,   6.40463582,   6.25444999,   7.80755028,\n",
       "         6.4399706 ,   6.50349834,   6.99347291,   6.22796269,\n",
       "         7.25557497,   5.96325784,   6.26440805,   7.53559052,\n",
       "         6.3716648 ,   6.46398327,   6.28978169,   7.26307978,\n",
       "         6.09396471,   7.25523109,   7.40718016,   6.53440396,\n",
       "         6.96895696,   6.18101776,   6.45762062,   7.17685642,\n",
       "         7.10440435,   6.75242034,   6.21377105,   6.80185624,\n",
       "         7.04259486,   6.5519317 ,   7.40244614,   6.22052199,\n",
       "         6.2258713 ,   7.03443882,   7.95509207,   6.18335941,\n",
       "         7.87423768,   7.69328503,   6.33027649,   6.43517016,\n",
       "         7.09999992,   6.53290315,   6.07756994,   7.2858363 ,\n",
       "         6.31166523,   6.21684548,   5.94480243,   6.85756544,\n",
       "         6.38674128,   7.5047407 ,   6.61111745,   6.87560609,\n",
       "         6.3311449 ,   6.88530699,   6.15509676,   7.46977755,\n",
       "         6.47205124,   6.51982085,   7.88991685,   6.19936908,\n",
       "         7.70475519,   6.67445871,   6.066422  ,   6.49299292,\n",
       "         6.15775485,   6.20918983,   6.21222529,   6.01776374,\n",
       "         7.13858496,   6.35658788,   6.49979529,   8.956749  ,\n",
       "         6.33066772,   6.21592609,   6.32178844,   6.88591267,\n",
       "         6.82082431,   6.45849293,   6.89677041,   6.81209425,\n",
       "         6.30034749,   6.96034049,   6.30921858,   6.50282972,\n",
       "         6.90751372,   6.66615918,   6.28451479,   6.84566805,\n",
       "         6.6283504 ,   6.22393615,   6.03740529,   6.35727848,\n",
       "         6.38749625,   6.32951364,   7.08604935,   6.8864782 ,\n",
       "         6.43192626,   6.25068897,   7.37258293,   6.53140583,\n",
       "         6.26538162,   6.87121386,   7.05889037,   6.63082948,\n",
       "         7.08313365,   6.16915744,   6.82312285,   7.6370249 ,\n",
       "         5.88277633,   6.83734861,   7.49796286,   6.65135172,\n",
       "         6.42408352,   6.46882454,   6.75924859,   6.73168756,\n",
       "         7.25668848,   6.02252553,   6.58184476,   6.47990334,\n",
       "         6.01325808,   7.07814346,   6.29070449,   6.36729859,\n",
       "         6.10194655,   6.96974023,   6.74125687,   6.21515783,\n",
       "         6.72474051,   6.16218498,   6.24683295,   6.61088339,\n",
       "         6.14910496,   6.66141516,   6.32069325,   6.97340247,\n",
       "         5.91713681,   6.43419503,   6.60576566,   7.11510252,\n",
       "         6.11958621,   6.35853912,   6.25222509,   6.36112665,\n",
       "         6.71893836,   6.72112521,   6.39226322,   6.23458561,\n",
       "         6.82138831,   6.2670083 ,   9.28336147,   6.49411311,\n",
       "         6.44201653,   6.13510251,   6.88062472,   6.44520087,\n",
       "         6.24571544,   6.15170446,   7.04648885,   6.05897877,\n",
       "         6.31925289,   6.33040806,   6.36609766,   6.80073194,\n",
       "         5.80277876,   6.0936906 ,   7.60568401,   6.76307307,\n",
       "         6.68342682,   6.54051378,   6.45045423,   6.26950341,\n",
       "         7.86924578,   6.22303463,   5.95362164,   7.0592681 ,\n",
       "         6.24454715,   6.2959564 ,   6.43778547,   6.19360088,\n",
       "         6.18998438,   6.96089543,   6.17654138,   6.52754528,\n",
       "         5.99714837,   6.71472532,   7.00932724,   6.37527307,\n",
       "         6.65602715,   6.65735924,   6.34986162,   6.03724489,\n",
       "         6.19233521,   6.73536396,   6.26304481,   6.62055094,\n",
       "         6.04929716,   5.9898702 ,   6.3544308 ,   6.48328466,\n",
       "         7.22559822,  10.19940522,   6.82098994,   6.91804822,\n",
       "         7.09602766,   6.99734224,   6.75310911,   6.49748508,\n",
       "         6.71253409,   6.00358294,   6.06213535,   6.42264156,\n",
       "         7.11326607,   6.28842398,   6.0602014 ,   6.56380335,\n",
       "         6.19651276,   6.57315835,   6.82710761,   6.14706761,\n",
       "         6.14544565,   5.94265646,   6.6696107 ,   6.53386809,\n",
       "         7.29457943,   6.27608738,   6.13899983,   5.90261435,\n",
       "         6.34566156,   6.12039066,   7.2003164 ,   6.44470374,\n",
       "         6.99804465,   6.6618607 ,   6.29744693,   7.72785704,\n",
       "         7.09324995,   6.94576212,   6.83219059,   6.38655619,\n",
       "         6.38184406,   6.12021602,   7.58148233,   6.94774997,\n",
       "         6.34363361,   6.47216854,   7.3034536 ,   6.4429959 ,\n",
       "         6.08882461,   6.79469349,   6.532814  ,   6.96449573,\n",
       "         7.08397314,   8.77838493,   6.46480272,   6.16984009,\n",
       "         6.62267005,   6.55235912,   6.71710825,   7.16813393,\n",
       "         6.66889508,   6.17201383,   6.14173722,   6.57407618,\n",
       "         7.18799495,   6.50255652,   6.15405282,   6.73267785,\n",
       "         6.43589431,   7.19817977,   6.31477982,   6.77882511,\n",
       "         6.03754863,   6.57007007,   7.22837322,   6.5685614 ,\n",
       "         7.14190474,   6.76670147,   6.84393355,   7.57901208,\n",
       "         6.39547783,   6.12175959,   8.26508357,   6.64668788,\n",
       "         6.57198009,   7.8362415 ,   7.00206163,   6.47154129,\n",
       "         6.97859621,   6.40339075,   6.53032564,   7.87355979,\n",
       "         6.46425462,   6.90843861,   6.11672659,   6.75611006,\n",
       "         6.42163293,   5.91834887,   8.10554344,   6.24360887,\n",
       "         6.37746605,   6.27269633,   7.12542684,   6.49421353,\n",
       "         7.04216913,   6.46307692,   6.63569889,   6.83856257,\n",
       "         6.49342298,   6.70434213,   6.24183876,   7.12837344,\n",
       "         6.80281047,   6.18307112,   6.47175162,   6.77367345,\n",
       "         7.09664082,   6.86345234,   6.30710347,   7.86925799,\n",
       "         6.35146436,   6.35292304,   6.57747683,   7.37591517,\n",
       "         7.19750487,   7.88329494,   7.54525796,   7.27130582,\n",
       "         6.06034717,   6.6485039 ,   6.72498493,   6.50070562,\n",
       "         6.51030718,   6.65924578,   7.09032788,   6.48325045,\n",
       "         6.43146842,   7.8989009 ,   6.92818192,   6.17030837,\n",
       "         6.30063099,   6.97738748,   7.46346909,   6.69930745,\n",
       "         6.14642091,   7.08075488,   6.36577768,   6.33353402,\n",
       "         6.27315883,   6.35079681,   6.77636773,   6.44969455,\n",
       "         6.56164433,   6.97630979,   6.60631207,   6.28111933,\n",
       "         7.43444363,   6.97489737,   6.35797164,   6.38515288,\n",
       "         6.39446267,   6.6195634 ,   6.88965671,   6.50332416,\n",
       "         7.84648345,   6.69103092,   7.13237556,   7.95602093,\n",
       "         6.01674979,   7.4202872 ,   6.5169456 ,   6.32445353,\n",
       "         6.2316845 ,   6.22999465,   6.38577869,   6.4789477 ,\n",
       "         6.20804442,   7.08141791,   6.46205914,   6.20493422,\n",
       "         6.7878201 ,   6.40283293,   6.5133414 ,   7.07206734,\n",
       "         6.7711167 ,   7.18069934,   7.12239273,   6.10003291,\n",
       "         6.35863949,   7.08413621,   7.00261832,   6.59787784,\n",
       "         6.21473669,   6.47154658,   7.18792797,   6.45745751,\n",
       "         6.59281329,   6.22615634,   6.81000971,   6.96224295,\n",
       "         8.05594794,   6.45640891,   6.57774573,   6.40259863,\n",
       "         6.09103125,   6.86416616,   7.12484322,   6.73003321,\n",
       "         6.58220927,   7.42746005,   6.40576804,   7.62854481,\n",
       "         6.64079991,   6.18572751,   7.29595648,   6.03377453,\n",
       "         7.11852196,   6.63461791,   6.74844741,   5.94250732,\n",
       "         6.40919105,   6.18556071,   6.49155685,   6.72549604,\n",
       "         6.06736475,   6.42774443,   6.2396238 ,   6.46176212,\n",
       "         6.45232031,   6.16014519,   6.27387672,   6.25701373,\n",
       "         6.3090881 ,   6.33641055,   6.01376478,   6.76963713,\n",
       "         6.67115459,   6.23630292,   7.15875514,   6.20502528,\n",
       "         6.47592967,   7.11359363,   6.22029652,   6.98851278,\n",
       "         7.48305332,   6.56741605,   6.9027735 ,   6.27196138,\n",
       "         6.20909859,   6.40510445,   6.38585638,   6.70734257,\n",
       "         6.53632536,   7.17900472,   7.20110003,   6.18058027,\n",
       "         5.8784051 ,   6.36958687,   6.62380132,   6.69797533,\n",
       "         6.91433319,   6.5702887 ,   6.54535031,   6.7408243 ,\n",
       "         6.7235072 ,   6.37195531,   6.64214464,   6.53964989,\n",
       "         6.26809209,   6.98765697,   6.9892495 ,   6.63702606,\n",
       "         6.6320139 ,   6.37750712,   6.08849188,   7.73634158,\n",
       "         6.63564942,   6.92026746,   6.22792473,   6.4575847 ,\n",
       "         6.70761391,   7.17846133,   6.59147797,   6.68402998,\n",
       "         8.2658937 ,   6.22186552,   6.51688809,   5.95735194,\n",
       "         6.73508738,   6.18398627,   6.14683506,   6.98542464,\n",
       "         6.68194806,   6.48131007,   8.39308225,   6.00244935,\n",
       "         6.55191932,   6.46219987,   6.22208092,   6.01296963,\n",
       "         8.76802649,   6.81913846,   6.11094128,   6.87559938,\n",
       "         6.65998307,   6.10778978,   7.92242676,   6.01051002,\n",
       "         6.28034795,   6.33649531,   7.14413726,   6.93306107,\n",
       "         6.52968741,   6.27450657,   6.09400026,   6.09053397,\n",
       "         6.18845567,   7.19554692,   6.62876242,   6.25076564,\n",
       "         6.40406817,   6.70278152,   8.85903172,   7.1573606 ,\n",
       "         6.32259839,   6.10915668,   6.50789579,   6.2211498 ,\n",
       "         6.07765269,   6.35194351,   7.27437081,   6.69305898,\n",
       "         6.19239013,   6.34710017,   6.59813035,   7.53084319,\n",
       "         6.35766912,   6.44108654,   6.2897422 ,   6.54123291,\n",
       "         6.0993424 ,   7.16670443,   6.97363927,   6.36061201,\n",
       "         6.31179622,   6.34485657,   7.33079053,   6.43243513,\n",
       "         6.04226431,   6.06808869,   6.84445831,   7.00261353,\n",
       "         5.97326218,   6.77137083,   7.21148324,   6.57338078,\n",
       "         6.70046247,   6.31163165,   6.43668732,   8.48591633,\n",
       "         6.15741899,   7.06791761,   7.26473602,   7.52790677,\n",
       "         7.52700117,   6.3919575 ,   6.03629502,   6.66493513,\n",
       "         6.77763246,   6.31858654,   6.20349747,   7.83676689,\n",
       "         6.58747392,   6.92301558,   6.70841812,   6.36147851,\n",
       "         6.26426524,   6.07406887,   6.93018381,   7.04900194,\n",
       "         7.35479171,   6.41340516,   6.6008398 ,   6.27379905,\n",
       "         5.87822433,   5.97864343,   5.9678574 ,   6.39713524,\n",
       "         6.5783513 ,   6.03236513,   6.2182413 ,   7.36958064,\n",
       "         6.45112085,   6.87239829,   6.42382419,   6.41949666,\n",
       "         6.73779155,   7.05720555,   6.61137288,   6.2784547 ,\n",
       "         6.65276968,   7.59706356,   6.37878741,   6.66237831,\n",
       "         6.11426845,   6.60684108,   6.55040282,   6.99645317,\n",
       "         7.41532678,   7.1073299 ,   8.20076668,   6.7545101 ,\n",
       "         6.65167414,   7.1196338 ,   6.62276719,   6.36500563,\n",
       "         6.53397042,   6.30690354,   7.71389962,   6.47670252,\n",
       "         6.39471083,   6.9669138 ,   6.69079965,   6.87716621,\n",
       "         6.61129879,   6.43100904,   7.09427758,   6.4513482 ,\n",
       "         6.21589802,   6.75432603,   6.85978601,   6.35854073,\n",
       "         6.2548871 ,   6.40358369,   6.31677678,   5.97890814,\n",
       "         6.25767429,   6.65133255,   7.55585941,   6.55012383,\n",
       "         6.32290548,   6.76222444,   6.48539561,   6.58515525,\n",
       "         6.58162188,   6.16868589,   6.55148228,   6.47191696,\n",
       "         6.42527009,   6.50921518,   6.12691428,   5.89894997,\n",
       "         7.02506151,   6.46398197,   7.1534405 ,   7.40185499,\n",
       "         6.5202642 ,   6.71624233,   6.07460218,   6.38163912,\n",
       "         6.81708847,   6.79003043,   6.46889673,   6.09671434,\n",
       "         6.15624949,   6.58703671,   6.03258113,   6.86293663,\n",
       "         6.10569994,   6.37263491,   6.94212055,   6.6755642 ,\n",
       "         6.19271488,   6.42681853,   6.65303183,   6.57977492,\n",
       "         7.32704159,   6.24242875,   6.47443379,   6.84070335,\n",
       "         6.22733097,   6.15484046,   6.85291437,   7.22664658,\n",
       "         7.4672466 ,   6.78261124,   6.34393659,   6.75021067,\n",
       "         6.42024558,   7.30302804,   6.49525952,   6.47216722,\n",
       "         6.11143325,   6.40763868,   6.77937271,   6.54317258,\n",
       "         6.89652559,   6.42867649,   6.20398135,   6.38813729])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##they don't match exactly, but differences are 6-7 orders of mag smaller than errors\n",
    "log10(abs(guh))-log10(abs(guh+grrrr[0,:,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Tv = 200\n",
    "training_model_space\n",
    "training_voxel_data\n",
    "validation_model_space\n",
    "validation_voxel_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop with early stopping, mini-batch sgd\n",
    "http://deeplearning.net/tutorial/gettingstarted.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#specify number of training batches\n",
    "n_train_batches = ...\n",
    "\n",
    "# early-stopping parameters\n",
    "patience = 5000  # look as this many examples regardless\n",
    "patience_increase = 2     # wait this much longer when a new best is\n",
    "                              # found\n",
    "improvement_threshold = 0.995  # a relative improvement of this much is\n",
    "                               # considered significant\n",
    "validation_frequency = min(n_train_batches, patience/2)\n",
    "                              # go through this many\n",
    "                              # minibatches before checking the network\n",
    "                              # on the validation set; in this case we\n",
    "                              # check every epoch\n",
    "\n",
    "                \n",
    "best_params = None\n",
    "best_validation_loss = numpy.inf\n",
    "test_score = 0.\n",
    "start_time = time.clock()\n",
    "\n",
    "done_looping = False\n",
    "epoch = 0\n",
    "while (epoch < n_epochs) and (not done_looping):\n",
    "    # Report \"1\" for first epoch, \"n_epochs\" for last epoch\n",
    "    epoch = epoch + 1\n",
    "    for minibatch_index in xrange(n_train_batches):\n",
    "\n",
    "        d_loss_wrt_params = compute_grad(voxel_data, model_space, feature_weights)  ##<<must supply minibatch index\n",
    "        params -= learning_rate * d_loss_wrt_params # gradient descent\n",
    "\n",
    "        # iteration number. We want it to start at 0.\n",
    "        iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "        # note that if we do `iter % validation_frequency` it will be\n",
    "        # true for iter = 0 which we do not want. We want it true for\n",
    "        # iter = validation_frequency - 1.\n",
    "        if (iter + 1) % validation_frequency == 0:\n",
    "            \n",
    "            prediction_menu = bigmult(validation_voxel_data,best_params)\n",
    "            this_validation_loss = sq_diff_func(voxel_data,prediction_menu) # compute zero-one loss on validation set\n",
    "\n",
    "            if this_validation_loss < best_validation_loss: ##must distribute condition across all voxels\n",
    "\n",
    "                # improve patience if loss improvement is good enough\n",
    "                if this_validation_loss < best_validation_loss * improvement_threshold:\n",
    "\n",
    "                    patience = max(patience, iter * patience_increase)\n",
    "                best_params = copy.deepcopy(params)\n",
    "                best_validation_loss = this_validation_loss\n",
    "\n",
    "        if patience <= iter:\n",
    "            done_looping = True\n",
    "            break\n",
    "\n",
    "# POSTCONDITION:\n",
    "# best_params refers to the best out-of-sample parameters observed during the optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
