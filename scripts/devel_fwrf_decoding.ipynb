{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### load / create a feature dictionary\n",
    "#### load / create a model space\n",
    "#### built a routine for unraveling the feature space\n",
    "#### build theano method for generating model-space tensors from unraveled feature spaces\n",
    "#### build theano method for generating predictions from unraveled fs\n",
    "#### build theano method for predicted/measured error across *voxels*, not trials\n",
    "#### build theano method for gradient w.r.t flattened feature maps\n",
    "#### refactor model-training machinery to run decoding with voxel resampling and early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from theano import tensor as tnsr\n",
    "from theano import function\n",
    "from theano import pp\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 3, 101)\n",
      "(10, 3, 101)\n"
     ]
    }
   ],
   "source": [
    "T = 10\n",
    "D = 3\n",
    "S = 20\n",
    "V = 101\n",
    "B = 12\n",
    "\n",
    "feature_map_tnsr = tnsr.tensor4('feature_map_tnsr') ##T x D x stim_size x stim_size\n",
    "\n",
    "population_rf_stack = tnsr.tensor3('pop_rf_stack_tnsr') # V x stim_size x stim_size\n",
    "\n",
    "feature_map_means = tnsr.matrix('feature_map_means') ##D x V\n",
    "feature_map_stdev = tnsr.matrix('feature_map_stdev') ##D x V\n",
    "\n",
    "population_model_space_tnsr = (tnsr.log(1+tnsr.sqrt(tnsr.tensordot(feature_map_tnsr,  ##T x D x S x S\n",
    "                         population_rf_stack,                   ##    V x S x S\n",
    "\t\t\t\t\t     axes = [[2,3],[1,2]],\n",
    "\t\t\t\t\t     ))) - feature_map_means) - feature_map_stdev\n",
    "\n",
    "pmst = function(inputs=[feature_map_tnsr, population_rf_stack, feature_map_means, feature_map_stdev], outputs = population_model_space_tnsr)\n",
    "##gives T x B x D x V\n",
    "\n",
    "\n",
    "fmt = np.random.rand(T,D,S,S).astype('float32')\n",
    "prfs = np.random.rand(V, S, S).astype('float32')\n",
    "fmm = np.random.rand(D,V).astype('float32')\n",
    "fms = np.random.rand(D,V).astype('float32')\n",
    "\n",
    "print pmst(fmt, prfs, fmm, fms).shape\n",
    "print (T, D, V)\n",
    "\n",
    "population_feature_weight_tnsr = tnsr.matrix('NU')\n",
    "population_prediction_tnsr = (population_model_space_tnsr*population_feature_weight_tnsr).sum(axis=1)    \n",
    "                                            \n",
    "                                            \n",
    "\n",
    "\n",
    "ppt = function(inputs = [population_model_space_tnsr, population_feature_weight_tnsr],\n",
    "              outputs = population_prediction_tnsr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 101)\n"
     ]
    }
   ],
   "source": [
    "pfwt = np.random.rand(D,V).astype('float32')\n",
    "X = pmst(fmt,prfs, fmm, fms)\n",
    "\n",
    "print ppt(X,pfwt).shape\n",
    "##gives T x V which is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "voxel_data_tnsr = tnsr.matrix('voxel_data_tnsr')\n",
    "population_prediction_error = tnsr.sqr(population_prediction_tnsr - voxel_data_tnsr).sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ppe_func = function(inputs=[population_prediction_tnsr, voxel_data_tnsr],\n",
    "                    outputs=population_prediction_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vdt = np.random.rand(T,V).astype('float32')\n",
    "ppe_func(ppt(X, pfwt), vdt).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##grad_population_prediction_error_wrt_feature_map\n",
    "gfm = tnsr.grad(population_prediction_error,feature_map_tnsr) ##<<the summing trick above makes this easy. \n",
    "\n",
    "compute_fm_grad = function(inputs = [voxel_data_tnsr,\n",
    "                                     feature_map_tnsr, population_rf_stack, feature_map_means, feature_map_stdev,\n",
    "                                     population_feature_weight_tnsr],\n",
    "                           outputs=gfm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 3, 20, 20)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_fm_grad(vdt, fmt, prfs, fmm, fms, pfwt).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-22-3bd07fe3bbd1>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-22-3bd07fe3bbd1>\"\u001b[1;36m, line \u001b[1;32m9\u001b[0m\n\u001b[1;33m    predict = theano.function(inputs=[x], outputs=prediction)\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "gradient_step_for_one_layer = function(inputs=[voxel_data_tnsr,\n",
    "                                               population_rf_stack, feature_map_means, feature_map_stdev,\n",
    "                                               population_feature_weight_tnsr],\n",
    "\n",
    "                 outputs=[population_prediction_tnsr, population_prediction_error],\n",
    "                 updates=((feature_map_tnsr, feature_map_tnsr - step_size * gfm), \n",
    "                          )\n",
    "\n",
    "predict = theano.function(inputs=[x], outputs=prediction)\n",
    "\n",
    "# Train\n",
    "for i in range(training_steps):\n",
    "    pred, err = train(D[0], D[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hoo = tnsr.tensor4('hoo')\n",
    "ha  = tnsr.tensor4('ha')\n",
    "\n",
    "hooha = tnsr.batched_tensordot(hoo, ha, axes=[[2,3],[2,3]])\n",
    "\n",
    "x = np.zeros((D, T, S, S)).astype('float32')\n",
    "y = np.zeros((D, V, S, S)).astype('float32')\n",
    "\n",
    "hooha_func = function(inputs = [hoo, ha], outputs = hooha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 10, 101)\n",
      "(3, 10, 101)\n"
     ]
    }
   ],
   "source": [
    "print hooha_func(x,y).shape\n",
    "print (D, T, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_vector = tnsr.vector('wv')\n",
    "\n",
    "\n",
    "a = tnsr.scalar('a')\n",
    "b = tnsr.scalar('b')\n",
    "c = tnsr.scalar('c')\n",
    "\n",
    "x_a = tnsr.scalar('x_a')\n",
    "x_b = tnsr.scalar('x_b')\n",
    "x_c = tnsr.scalar('x_c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_expr = [0]\n",
    "indices = [0,1,2]\n",
    "for ii, inp in enumerate([x_a,x_b,x_c]):\n",
    "    p_expr.append(p_expr[-1]+weight_vector[indices[ii]]*inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " Elemwise{add,no_inplace}.0,\n",
       " Elemwise{add,no_inplace}.0,\n",
       " Elemwise{add,no_inplace}.0]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(((TensorConstant{0} + (Constant{0}[wv] * x_a)) + (Constant{1}[wv] * x_b)) + (Constant{2}[wv] * x_c))'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pp(p_expr[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "foo = tnsr.jacobian(p_expr[-1],weight_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "baz = function(inputs=[weight_vector,x_a,x_b,x_c], outputs=foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  2.,  3.], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_a,in_b,in_c = np.array(1,dtype='float32'),np.array(2,dtype='float32'),np.array(3,dtype='float32')\n",
    "wv = np.array([10,20,30],dtype='float32')\n",
    "baz(wv, in_a,in_b,in_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layer_expr = []\n",
    "for inp,wt in zip([a,b,c],[x_a, x_b, x_c]):\n",
    "    layer_expr.append(inp*wt)\n",
    "final_expr = tnsr.stack(layer_expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'join(TensorConstant{0}, [(a * x_a), (b * x_b), (c * x_c)])'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp(final_expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blarg = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class feature_map(object):\n",
    "    def __init__(self, shape, name):\n",
    "        self.shape = shape #(time x depth x resolution x resolution)\n",
    "        self.name = name\n",
    "        self.f_map_tnsr = tnsr.tensor4(name)\n",
    "\n",
    "        \n",
    "class encoding_model(object):\n",
    "    def __init__(self, feature_dictionary, activation_function, rf_instance):\n",
    "        self.f_maps = [feature_map(feature_dictionary[key].shape,key) for key in feature_dictionary.keys()]\n",
    "        self.receptive_fields = rf_instance\n",
    "        self.weight_tnsr = tnsr.matrix('weight_matrix')  ##voxels x features (no space). shared?\n",
    "        self.neural_tnsr = tnsr.tensor4('neural')\n",
    "        self._build_feature_index()\n",
    "        self._build_activation_expressions()\n",
    "        self.prediction = function(inputs=[],outputs=[self.prediction_expr]) ##mean/stdev should be givens\n",
    "        self.cost = function(inputs=[],outputs=[self.cost_expr])\n",
    "        \n",
    "    #=====================================================================================================\n",
    "    #===this should all go into the model space class...most of it already is...\n",
    "    #===we just need a more nimble class that builds its own expressions for use by \n",
    "    #===by other objects, and that allows the model-space tensor to be built piece-wise.\n",
    "    def _build_feature_index(self):\n",
    "        print 'build it yourself'\n",
    "        ##dictionary of indices into weight matrix\n",
    "        \n",
    "        \n",
    "    ##apply rf and act. func. expressions\n",
    "    ##this creates a list expressions, one for each feature map\n",
    "    def _build_activation_expressions(self):\n",
    "        expr_accumulator = []\n",
    "        for ii,fm in enumerate(self.f_maps):\n",
    "            apply_rf = theano_expr\n",
    "            apply_act_fun = theano_expr\n",
    "            expr_accumulator.append(divide_out_stdev)  ##need to be careful about mutability here\n",
    "        self._model_space_expressions = expr_accumulator\n",
    "    \n",
    "    ##load in feature dictionary. \n",
    "    def _construct_activation_tensor(self, feature_dictionary):\n",
    "        ##this will apply the rfs and act. funcs to actual data\n",
    "        ##for loop over time_chunks:\n",
    "            ##construct rf_stack for specified indices\n",
    "            ##apply rf stack\n",
    "        \n",
    "        ##return activation tensor\n",
    "    \n",
    "        \n",
    "    def _get_feature_map_stats(self, feature_dictionary):\n",
    "        print 'get them yourself'\n",
    "        ##this will calculate mean/stdev for each (feature_map , rf) channel\n",
    "        ##if the stdev is too small, the indices for the (feature_map, rf) channel\n",
    "        ##are recorded and will be ignored during the gradient step. in this case,\n",
    "        ##mean=0 and stdev=1. otherwise we just apply mean/stdev normally.\n",
    "        \n",
    "    def _build_model_space_expressions(self):\n",
    "#         apply mean/stdev and the feature weights to get a prediction expression\n",
    "#         subtract_mean = theano_expr\n",
    "#         divide_out_stdev = theano_expr\n",
    "#         sub-select \n",
    "        self._model_space_tnrs_expr = theano_expr\n",
    "        \n",
    "    def _construct_model_space_tensor(self):\n",
    "        self._construct_activation_tensor(feature_dictionary)\n",
    "        \n",
    "    ##==================================================================================================    \n",
    "        \n",
    "    def _build_prediction_expression(self):\n",
    "          ##builds on the expressions for the model space. applies the weights  \n",
    "            ##this won't even be used during training. just for validation, etc. after training is done.\n",
    "#         apply_feature_weights = theano_expr  ##here we need weight matrix indices for current f_map\n",
    "#         expr_accumulator.append(expr_accumulator[-1]+apply_feature_weights)\n",
    "\n",
    "#         self.prediction_expr = expr_accumulator[-1]\n",
    "#         print 'prediction expression built'\n",
    "#         pp(self.prediction_expr)\n",
    "        \n",
    "\n",
    "    \n",
    "    def _build_cost_expr(self):\n",
    "        ##uses the same \"apply weight\" expression for the prediction menu as above\n",
    "        neural_tnsr = theano_expr\n",
    "        prediction_menu_tnsr = theano_expr\n",
    "        self.cost_expr = tnsr.sqr(prediction_menu_tnsr - neural_tnsr).sum() \n",
    "        \n",
    "    \n",
    "    def _build_grad_expr(self):\n",
    "        self.grad_expr = tnsr.grad(self.cost_expr, self.weight_tnsr)\n",
    "        \n",
    "\n",
    "        \n",
    "    def _build_training_kernel(self):\n",
    "        ##returns a function that applies gradient to feature weights then outputs updated feat. weights.\n",
    "         self.training_kernel = theano_expr\n",
    "        print 'train your own damn kernel'\n",
    "\n",
    "    \n",
    "    def _prepare_feature_maps(self, feature_dictionary, append_to=None):\n",
    "        ##call this to construct the model-space tensor. \n",
    "        ##call this in a loop with append=True if feature dict. doesn't fit into memory,\n",
    "    \n",
    "    \n",
    "    def initialize_training(self, feature_maps, neural_data, initial_weights='zeros', prepared = False, chunking_params):\n",
    "        ##constructs the model_space tensor. \n",
    "        ##if prepared = True, insert a model-space tensor for feature maps\n",
    "        ##otherwise, call \"prepared_feature_maps\"\n",
    "        ##returns indices for splitting up \n",
    "    \n",
    "    def train_me(self):\n",
    "        for v in voxel_list:\n",
    "            trn_activity = subset_of_data\n",
    "            for g in rf_grid_list:\n",
    "                mst = self.model_space.contruct_mst()\n",
    "        \n",
    "        print 'train it yourself'\n",
    "        \n",
    "    def test_me(self, val_activity):\n",
    "        print 'test it yourself'\n",
    "\n",
    "        \n",
    "class decoder(object):\n",
    "    def __init__(self, encoding_model_instance):\n",
    "        self.encoding_model=encoding_model_instance\n",
    "        print 'init yourself'\n",
    "        \n",
    "    def _build_cost_expr(self):\n",
    "        self.cost = tnsr.sqr(neural_tnsr - self.prediction_expression).sum() ##or something like this...\n",
    "    \n",
    "    def _build_gradient_expr(self):\n",
    "        self.gradient_expr = jacobian(self.prediction_expr,wrt=[ fm.f_map_tnsr for fm in self.encoding_model.f_maps])\n",
    "        \n",
    "    \n",
    "    def _build_decoding_kernel(self):\n",
    "        ##\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "def gradient_descender(data, init_weights, gradient_update_func, pred_func, cost_func):\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
