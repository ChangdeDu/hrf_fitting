{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use feature-weighted rf model on the crcns vim-1 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN X\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from time import time\n",
    "from glob import glob\n",
    "from scipy.io import loadmat\n",
    "from PIL import Image\n",
    "from scipy.stats import pearsonr\n",
    "from hrf_fitting.src.feature_weighted_rf_models import make_rf_table,receptive_fields, model_space, prediction_menu, bigmult\n",
    "from hrf_fitting.src.feature_weighted_rf_models import train_fwrf_model\n",
    "from hrf_fitting.src.gabor_feature_dictionaries import gabor_feature_maps\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Load crcns stimuli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load crcns stimuli "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##known stimulus parameters\n",
    "Ttrn = 1750\n",
    "Tval = 120\n",
    "S = 500\n",
    "T = Ttrn+Tval\n",
    "train_stim_files = glob('/media/tnaselar/Data/crcns_datasets/vim-1/Stimuli_Trn_FullRes*.mat')\n",
    "val_stim_file = '/media/tnaselar/Data/crcns_datasets/vim-1/Stimuli_Val_FullRes.mat'\n",
    "n_image_channels = 1 ##could be 3 for color images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##allocate memory for stim\n",
    "training_stim = np.zeros((Ttrn,S,S),dtype='float32')\n",
    "\n",
    "##load training stim\n",
    "cnt = 0\n",
    "for sl in train_stim_files:\n",
    "    this_h5 = h5py.File(sl,'r')\n",
    "    this_train_stim = this_h5['stimTrn']\n",
    "    this_num_stim = this_train_stim.shape[-1]\n",
    "    training_stim[cnt:cnt+this_num_stim,:,:] = np.transpose(this_train_stim[:],[2,1,0])\n",
    "    cnt += this_num_stim\n",
    "    this_h5.close()\n",
    "    \n",
    "##load validation stim\n",
    "val_h5 = h5py.File(val_stim_file,'r')\n",
    "validation_stim = np.transpose(val_h5['stimVal'][:],[2,1,0])\n",
    "val_h5.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(validation_stim[0,:,:],cmap='gray')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(training_stim[0,:,:],cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: construct feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_orientations = 8\n",
    "deg_per_stimulus = 20\n",
    "lowest_sp_freq = .2 ##cyc/deg\n",
    "highest_sp_freq = 1.5\n",
    "num_sp_freq = 8\n",
    "pix_per_cycle = 2.13333333\n",
    "complex_cell = True\n",
    "\n",
    "print 'D = total number of features = %d' %(n_orientations * num_sp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### construct gabor wavelet stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gfm = gabor_feature_maps(n_orientations,\n",
    "                         deg_per_stimulus,\n",
    "                         (lowest_sp_freq,highest_sp_freq,num_sp_freq),\n",
    "                         pix_per_cycle=pix_per_cycle,complex_cell=complex_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gfm.gbr_table.head(num_sp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gfm.filter_stack.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### see one of the gabors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "o = 1 ##choose an orientation\n",
    "plt.imshow(np.real(gfm.filter_stack[o,0,:,:]),cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: receptive fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deg_per_radius = (0.5, 5, 6) ##rf sizes in degrees (smallest, largest, number of sizes)\n",
    "spacing = 1. ##spacing between rf's in degrees\n",
    "rf = receptive_fields(deg_per_stimulus,deg_per_radius,spacing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf.rf_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'G = number of rf models = %d' %(rf.rf_table.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Model space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### instantiate model space object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##construct a sample feature dict to initiate model space\n",
    "##note that gfm expects stimuli to be 4D T x n_color_channels x S x S tensors,\n",
    "##so if we want to pass just one image we need to add two singleton axes\n",
    "init_feat_dict = gfm.create_feature_maps(training_stim[0,np.newaxis,np.newaxis,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##construct the model space, add a bias feature (all 1's)\n",
    "ms = model_space(init_feat_dict, rf, add_bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### construct training/validation model space tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##loop over training stimuli because feature maps for all training stim. > 48Gb\n",
    "training_mst = np.zeros((ms.receptive_fields.G, Ttrn, ms.D)).astype('float32')\n",
    "\n",
    "num_chunks = 2\n",
    "stim_dx = np.linspace(0,T-1,num=num_chunks+1, endpoint=True,dtype='int')\n",
    "\n",
    "cnt = 0\n",
    "for t in stim_dx[1:]:\n",
    "    this_training_feature_dict = gfm.create_feature_maps(training_stim[cnt:cnt+t,np.newaxis,:,:])\n",
    "    training_mst[:,cnt:cnt+t,:] = ms.construct_model_space_tensor(this_training_feature_dict,normalize=False)\n",
    "    cnt += t\n",
    "\n",
    "##clear up memory\n",
    "this_training_feature_dict = []\n",
    "\n",
    "##normalize and save normalization constants\n",
    "training_mst = ms.normalize_model_space_tensor(training_mst, save=True)\n",
    "\n",
    "\n",
    "##should work in one shot for because not too big\n",
    "val_feature_dict = gfm.create_feature_maps(validation_stim[:,np.newaxis,:,:])\n",
    "validation_mst = ms.construct_model_space_tensor(val_feature_dict)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: load and package crcns voxel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "voxel_file = '/media/tnaselar/Data/crcns_datasets/vim-1/EstimatedResponses.mat'\n",
    "crcns_voxel_data = h5py.File(voxel_file,'r')\n",
    "crcns_voxel_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### remove nans, becuase this data-set has some. otherwise even one nan will infect gradient for every voxel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "voxel_data = np.concatenate((crcns_voxel_data['dataValS1'],crcns_voxel_data['dataTrnS1']),axis=0).astype('float32')\n",
    "no_nan = np.isnan(voxel_data).sum(axis=0) == 0 ##<<only pulled voxels with nans in training data, should pull if nans in val data too.\n",
    "voxel_data = voxel_data[:,no_nan]\n",
    "print voxel_data.shape\n",
    "V = voxel_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crcns_voxel_data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get training/validation views on voxel_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nvox=1000\n",
    "trnIdx = np.arange(Tval,T)\n",
    "valIdx = np.arange(0,Tval)\n",
    "trn_voxel_data = voxel_data[trnIdx,0:nvox]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: run that shit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initialize the feature weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "initial_feature_weights = np.zeros((ms.receptive_fields.G,ms.D,nvox),dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fvl,ffw,frf,beh = train_fwrf_model(training_mst,\n",
    "                 trn_voxel_data,\n",
    "                 initial_feature_weights,\n",
    "                 voxel_binsize = nvox,\n",
    "                 rf_grid_binsize=100,\n",
    "                 learning_rate=10**(-4.9),\n",
    "                 max_iters = 100,\n",
    "                 early_stop_fraction=0.2,\n",
    "                 report_every = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss histories, all voxels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_=plt.plot(beh-beh[0,:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### view loss history for a few voxels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_=plt.plot(beh[:,slice(0,-1,20)]-beh[0,slice(0,-1,20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##loss in \"final_validation_loss\" = last point of \"best_error_history\"\n",
    "print np.min(beh[:,-2])\n",
    "print fvl[-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### diff between first and last point of loss history, all voxels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_=plt.hist(beh[0,:]-np.min(beh,axis=0),100)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: model analysis and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### histogram of rf models selected for each voxel\n",
    "models with large indices are small rfs. so looks like we need higher resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_=plt.hist(frf,ms.receptive_fields.G)\n",
    "plt.xlabel('bigger-->smaller')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sum of all selected rfs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(np.sum(ms.receptive_fields.make_rf_stack(64, min_pix_per_radius=1)[frf,:,:], axis=0), cmap='hot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prediction accuracy for all voxels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##grab validation data\n",
    "val_voxel_data = voxel_data[valIdx,0:nvox]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##generate predictions one voxel at a time\n",
    "pred = prediction_menu(validation_mst, ffw[np.newaxis,:,:], rf_indices = frf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##get correlation = prediction accuracy\n",
    "val_cc = []  \n",
    "for v in range(nvox-1): \n",
    "    cc = pearsonr(val_voxel_data[:,v],pred[:,v])\n",
    "    if not np.isnan(cc[0]):\n",
    "        val_cc.append(cc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##histogram of prediction accuracy, all voxels\n",
    "_=plt.hist(val_cc,100)\n",
    "plt.yscale('log')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
