{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature map models with time-space-feature-seperable weighting functions\n",
    "Here we're going to code up models with the following form:\n",
    "\n",
    "$$r_t = \\sum_{i}\\sum_{\\tau}\\sum_{x,y} \\phi_{i}(s(t-\\tau,x,y))w(\\tau,x,y; \\theta))\\nu_{i}+\\epsilon$$\n",
    "\n",
    "Where $r_t$ is the response of some neural unit at time $t$, $(x,y)$ are spatial coordinates, and $\\phi_i$ is a nonlinear feature map that is a function of a movie $s(t-\\tau,x,y)$. The adjustable parameters are $\\theta$, which specifies the space-time kernel $w$, and a set of independent features weights $\\nu_{i}$.\n",
    "\n",
    "In this model, we assume that the neuron views each feature map through the same spatial and temporal window $w$, but that it weights each feature map $\\phi_i$ according to it's importance $\\nu_i$. This is the fundamental assumption and it seems to be pretty valid, given what I know about vision at least.\n",
    "\n",
    "Our goal will be to infer the space-time kernel and feature weights that best predict measured activity $r$. We can perform this optimization for many voxels/neurons. The feature maps will be the same for each voxel/neuron, but the space-time kernel and feature weights will change according to the characteristics of each.\n",
    "\n",
    "In this version of the model feature and space-time selectivity are separable. Also, the space-time kernel is written as a parameterized function, while the feature weights are completely flexible--i.e., there is one independent feature weight per feature map. We can consider other variants, for example, where space and time are also separable, and where the feature weights are also a parameterized function of the features:\n",
    "\n",
    "$$r_t = \\sum_{i}\\sum_{\\tau}\\sum_{x,y} \\phi_{i}(s(t-\\tau,x,y))g(x,y; \\theta_{space}))f(\\tau; \\theta_{time}) \\nu(i; \\theta_{feature})+\\epsilon$$ \n",
    "\n",
    "where $g$, $f$, and $\\nu$ are functions of space, time, and feature, respectively.\n",
    "\n",
    "An interesting variant would add an activation nonlinearity before application of the feature weights:\n",
    "\n",
    "$$r_t = \\sum_{i}\\sum_{\\tau}\\sum_{x,y} act(\\phi_{i}(s(t-\\tau,x,y))g(x,y; \\theta_{space}))f(\\tau; \\theta_{time}); exponent) \\nu(i; \\theta_{feature})+\\epsilon$$ \n",
    "\n",
    "The more we separate, the more interpretable the model becomes, and the fewer parameters we have to fit. The crucial separation is space/feature, so that the number of parameters = *number of pixels* + *number of features*, instead of the product of these two terms. The more we separate, of course, the more likely we are to miss potentially important interactions between space, time, and feature.\n",
    "\n",
    "Currently the code is being designed to accomodate two kinds of optimiziation: a brute force grid search (the G dimension below) and some kind of regression or gradient-based procedure. The idea is that there are only a few RF kernel parameters so we can probably optimize these via a stupid brute-force search. Meanwhile, there are likely to be many feature map weights, so we optimize these using regression or gradient methods. The potentially nice thing about brute-forcing the RF params is that we won't have to do an iterative optimization procedure (in principle). Instead we regress over all possible RFkernels, and take the best one. It's an empirical question what params are best brute-forced, and which are best folded into the gradient descent procedure. The goal, however, is to not resort to an iterative, EM-like scheme where we have to assume values of one set of parameters in order to optimize the other set, and so forth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylab import has clobbered these variables: ['diff']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from theano import tensor as tnsr\n",
    "from theano import function, scan\n",
    "from time import time\n",
    "\n",
    "\n",
    "from hrf_fitting.src.features import make_complex_gabor as gaborme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some code for making feature maps.\n",
    "For example, with Gabors.\n",
    "This stage produces the feature maps =  T x D tensor. \n",
    "Question: normalize w.r.t. feature maps, or w.r.t. pixels within feature maps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cycles per deg.</th>\n",
       "      <th>cycles per pixel</th>\n",
       "      <th>cycles per stimulus</th>\n",
       "      <th>fwhm (pix)</th>\n",
       "      <th>n_pix</th>\n",
       "      <th>pixels per cycle</th>\n",
       "      <th>prf_size (deg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.010008</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>0.221184</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>3472.222222</td>\n",
       "      <td>11.050042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.331119</td>\n",
       "      <td>0.009528</td>\n",
       "      <td>7.317760</td>\n",
       "      <td>104.950149</td>\n",
       "      <td>262.375372</td>\n",
       "      <td>104.950149</td>\n",
       "      <td>1.510031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.652230</td>\n",
       "      <td>0.018769</td>\n",
       "      <td>14.414336</td>\n",
       "      <td>53.280290</td>\n",
       "      <td>133.200725</td>\n",
       "      <td>53.280290</td>\n",
       "      <td>0.766601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.973341</td>\n",
       "      <td>0.028009</td>\n",
       "      <td>21.510912</td>\n",
       "      <td>35.702810</td>\n",
       "      <td>89.257025</td>\n",
       "      <td>35.702810</td>\n",
       "      <td>0.513695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.294452</td>\n",
       "      <td>0.037249</td>\n",
       "      <td>28.607488</td>\n",
       "      <td>26.846118</td>\n",
       "      <td>67.115295</td>\n",
       "      <td>26.846118</td>\n",
       "      <td>0.386264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.615562</td>\n",
       "      <td>0.046490</td>\n",
       "      <td>35.704064</td>\n",
       "      <td>21.510156</td>\n",
       "      <td>53.775391</td>\n",
       "      <td>21.510156</td>\n",
       "      <td>0.309490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.936673</td>\n",
       "      <td>0.055730</td>\n",
       "      <td>42.800640</td>\n",
       "      <td>17.943657</td>\n",
       "      <td>44.859142</td>\n",
       "      <td>17.943657</td>\n",
       "      <td>0.258175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.257784</td>\n",
       "      <td>0.064970</td>\n",
       "      <td>49.897216</td>\n",
       "      <td>15.391640</td>\n",
       "      <td>38.479101</td>\n",
       "      <td>15.391640</td>\n",
       "      <td>0.221456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.578895</td>\n",
       "      <td>0.074211</td>\n",
       "      <td>56.993792</td>\n",
       "      <td>13.475152</td>\n",
       "      <td>33.687880</td>\n",
       "      <td>13.475152</td>\n",
       "      <td>0.193881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.900006</td>\n",
       "      <td>0.083451</td>\n",
       "      <td>64.090368</td>\n",
       "      <td>11.983080</td>\n",
       "      <td>29.957700</td>\n",
       "      <td>11.983080</td>\n",
       "      <td>0.172413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cycles per deg.  cycles per pixel  cycles per stimulus  fwhm (pix)  \\\n",
       "0         0.010008          0.000288             0.221184  768.000000   \n",
       "1         0.331119          0.009528             7.317760  104.950149   \n",
       "2         0.652230          0.018769            14.414336   53.280290   \n",
       "3         0.973341          0.028009            21.510912   35.702810   \n",
       "4         1.294452          0.037249            28.607488   26.846118   \n",
       "5         1.615562          0.046490            35.704064   21.510156   \n",
       "6         1.936673          0.055730            42.800640   17.943657   \n",
       "7         2.257784          0.064970            49.897216   15.391640   \n",
       "8         2.578895          0.074211            56.993792   13.475152   \n",
       "9         2.900006          0.083451            64.090368   11.983080   \n",
       "\n",
       "        n_pix  pixels per cycle  prf_size (deg)  \n",
       "0  768.000000       3472.222222       11.050042  \n",
       "1  262.375372        104.950149        1.510031  \n",
       "2  133.200725         53.280290        0.766601  \n",
       "3   89.257025         35.702810        0.513695  \n",
       "4   67.115295         26.846118        0.386264  \n",
       "5   53.775391         21.510156        0.309490  \n",
       "6   44.859142         17.943657        0.258175  \n",
       "7   38.479101         15.391640        0.221456  \n",
       "8   33.687880         13.475152        0.193881  \n",
       "9   29.957700         11.983080        0.172413  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##parameters for designing gabor feature maps \n",
    "pixel_per_deg = 34.751    ##determined by experiment 34.751 = match/nonmatch exp.\n",
    "pixels_per_stimulus = 768 ##det. by exp. 768 = match/nonmatch exp.\n",
    "cycles_per_pixel = np.linspace(0.000288,0.083451,num=10)  ##cyc/pix\n",
    "cycles_per_fwhm = 1.0\n",
    "fwhms_per_kernel = 2.5 ##determines how big the gabor should be.\n",
    "\n",
    "metrics = {'cycles per pixel':cycles_per_pixel,\n",
    "           'pixels per cycle': 1./cycles_per_pixel,\n",
    "           'cycles per stimulus': cycles_per_pixel*pixels_per_stimulus,\n",
    "           'cycles per deg.': cycles_per_pixel*pixel_per_deg,\n",
    "           'fwhm (pix)': np.clip(cycles_per_fwhm/cycles_per_pixel,0,pixels_per_stimulus),\n",
    "           'prf_size (deg)': np.clip(cycles_per_fwhm/cycles_per_pixel,0,pixels_per_stimulus)/pixel_per_deg/2.,\n",
    "           'n_pix': np.clip(fwhms_per_kernel*cycles_per_fwhm/cycles_per_pixel,0,pixels_per_stimulus)}\n",
    "\n",
    "fm = pd.DataFrame(metrics)\n",
    "fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx = 9\n",
    "ori = np.pi/2.\n",
    "center = (0,0)\n",
    "freq = fm.loc[idx,'cycles per stimulus']\n",
    "fwhm = fm.loc[idx,'fwhm (pix)']\n",
    "n_pix = int(fm.loc[idx,'n_pix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29, 29)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPwAAAD8CAYAAABTq8lnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n37DhjPD/AVzawY4rkf/1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd29ca61f50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "foo = gaborme(freq,ori,center,fwhm,n_pix)\n",
    "plt.imshow(np.real(foo),cmap='gray')\n",
    "print foo.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply RF kernels in a grid\n",
    "Takes the feature maps (T x D tensor) and applies multiple RF kernels. \n",
    "Produces a G x T x D tensor that we refer to as the \"model space\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Space-time kernel function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def space_time_kernel(center, fwhm, n_pix, time_kernel):\n",
    "    '''\n",
    "    a separable space-time kernel.\n",
    "    space kernel is gaussian\n",
    "    time kernel is an array of length T\n",
    "    returns a 3-D volume that is (n_pix,n_pix,T)\n",
    "    slice it to see what it does.\n",
    "    '''\n",
    "    space_kernel = np.atleast_3d(make_gaussian(center,fwhm,n_pix))\n",
    "    return time_kernel*space_kernel\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### construct a brute-force grid over spatial and temporal kernel and activation function params."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of feature-weight tensor to model-space tensor in Theano\n",
    "Something like this is what we'd need to generate predictions before we've decided what\n",
    "the RF params are going to be. So this code generates predictions for all RFs, all voxels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Some fake dimensions for testing\n",
    "D = 1000  ##number of feature maps\n",
    "T = 1100  ##number of data samples\n",
    "G = 100   ##coarse grid of r.f. centers/positions\n",
    "V = 100   ##voxels (chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = tnsr.tensor3('X')    ##model-space tensor: G x T x D\n",
    "NU = tnsr.tensor3('NU')  ##feature weight tensor: G x D x V\n",
    "Z = tnsr.batched_tensordot(X,NU,axes=[[2],[1]]) ##prediction tensor: G x T x V\n",
    "bigmult = function([X,NU], Z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model_space = np.random.rand(G,T,D).astype('float32')    ##RFparams x Trials x Dimensions\n",
    "feature_weights = np.random.rand(G,D,V).astype('float32') ##RFparams x Dimensions x Voxels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1100, 100)\n",
      "9.10070991516\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "print np.einsum('gtd,gdv->gtv',model_space,feature_weights).shape  ##RFparams x Trials x Voxels\n",
    "print time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Error allocating 40000000 bytes of device memory (out of memory).\nApply node that caused the error: GpuFromHost(NU)\nInputs types: [TensorType(float32, 3D)]\nInputs shapes: [(100, 1000, 100)]\nInputs strides: [(400000, 400, 4)]\nInputs values: ['not shown']\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-144-8366ffd12625>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mbigmult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_space\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeature_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/tnaselar/anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    604\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthunks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m                         storage_map=self.fn.storage_map)\n\u001b[0m\u001b[0;32m    607\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m                     \u001b[1;31m# For the c linker We don't have access from\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/tnaselar/anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    593\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    596\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Error allocating 40000000 bytes of device memory (out of memory).\nApply node that caused the error: GpuFromHost(NU)\nInputs types: [TensorType(float32, 3D)]\nInputs shapes: [(100, 1000, 100)]\nInputs strides: [(400000, 400, 4)]\nInputs values: ['not shown']\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "print bigmult(model_space,feature_weights).shape\n",
    "print time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.203444444444445"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".32*(64**2*8*8/G)*(165000/V)/60./60./24."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262144"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64**3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "^^^ For basic operation (data x weights) theano can be about x100 faster, although this seems to depend upon what else you're doing with the computer at the time. Seems important to start \"fresh\", with nothing else in memory.\n",
    "\n",
    "For 400K voxels and grid size of (centers x widths x activation exponent) = (64^2 x 8 x 8) we're looking at at least 190hrs on the GPU. Probably will take more time because this doesn't include the feature-weight optimization.\n",
    "\n",
    "If we are going to do grad. desc. on the feature weights we could potentially fold some of the RF or activation function parameters into the gradient..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection over brute-force grid (G)\n",
    "Calculate error over grid, then argmax it to generate final model for each voxel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### some fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "voxel_data = np.random.rand(T,V).astype('float32')\n",
    "prediction_menu = np.random.rand(G,T,V).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the error function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = tnsr.matrix('y')  ##voxel data tensor: T x V\n",
    "#y_hat = tnsr.tensor3('y_hat')\n",
    "\n",
    "diff = y-Z  ##difference tensor: (T x V) - (G x T x V) = (G x T x V)\n",
    "sq_diff = (diff*diff).sum(axis=1) ##sum-sqaured-diffs tensor: G x V\n",
    "sq_diff_func = function([y,Z],sq_diff)  ##what will this do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 500)\n"
     ]
    }
   ],
   "source": [
    "error_menu = sq_diff_func(voxel_data,prediction_menu)\n",
    "print error_menu.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_idx = error_menu.argmax(axis=0)\n",
    "best_model_idx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Gradient w.r.t. feature weights for many voxels at once.\n",
    "Can we do coor descent?\n",
    "How long does this take per voxel?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define a gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = tnsr.ivector('i')\n",
    "j = tnsr.ivector('j')\n",
    "matr = tnsr.matrix('matr')\n",
    "\n",
    "plus_one, updates = scan(fn = lambda row,col,menu : menu[row,col]+1, \n",
    "                  outputs_info = None,\n",
    "                  sequences = [i,j],\n",
    "                  non_sequences = matr)\n",
    "out_matr = plus_one.reshape(matr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = function(inputs=[i,j,matr], outputs=[out_matr], updates=updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.,  1.],\n",
       "        [ 1.,  1.]]], dtype=float32)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx,jdx=np.mgrid[0:2,0:2].astype('int32')\n",
    "idx = idx.ravel()\n",
    "jdx = jdx.ravel()\n",
    "foo = np.random.rand(2,2).astype('float32')\n",
    "f(idx,jdx,foo) - foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(0).astype('int32').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i_1 = tnsr.iscalar('i_1')\n",
    "i_2 = tnsr.iscalar('i_2')\n",
    "sqd = sq_diff[i_1,i_2]\n",
    "foo = function(inputs=[i_1,i_2,X,NU,y],outputs=[sqd])\n",
    "f_1 = int32(0)\n",
    "f_2 = int32(1)\n",
    "foo(f_1,f_2,model_space,feature_weights,voxel_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = tnsr.grad(sq_diff[i_1,i_2],NU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "J, updates = scan(lambda row,col,sq_diff,NU : tnsr.grad(sq_diff[row,col], NU),\n",
    "                  sequences=[i,j],\n",
    "                  non_sequences=[sq_diff,NU])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compute_grad = function(inputs=[i,j,X,NU,y],\n",
    "                  outputs = [J],\n",
    "                  updates=updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Error allocating 220000000 bytes of device memory (out of memory).\nApply node that caused the error: GpuElemwise{mul,no_inplace}(GpuDimShuffle{0,x,1}.0, <CudaNdarrayType(float32, 3D)>)\nInputs types: [CudaNdarrayType(float32, (False, True, False)), CudaNdarrayType(float32, 3D)]\n\nHINT: Use another linker then the c linker to have the inputs shapes and strides printed.\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.\nApply node that caused the error: for{gpu,scan_fn}(Elemwise{minimum,no_inplace}.0, Subtensor{int64:int64:int8}.0, Subtensor{int64:int64:int8}.0, Elemwise{minimum,no_inplace}.0, GpuCAReduce{pre=sqr,red=add}{0,1,0}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, Elemwise{minimum,no_inplace}.0, ScalarFromTensor.0, GpuElemwise{sub,no_inplace}.0, GpuElemwise{sqr,no_inplace}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0)\nInputs types: [TensorType(int64, scalar), TensorType(int32, vector), TensorType(int32, vector), TensorType(int64, scalar), CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, 3D), TensorType(int64, scalar), Scalar(int64), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D)]\nInputs shapes: [(), (110000,), (110000,), (), (100, 500), (100, 500), (100, 1000, 500), (), (), (100, 1100, 500), (100, 1100, 500), (100, 1100, 500), (100, 1000, 500), (100, 1100, 1000)]\nInputs strides: [(), (4,), (4,), (), (500, 1), (500, 1), (500000, 500, 1), (), (), (550000, 500, 1), (550000, 500, 1), (-550000, 500, 1), (-500000, 500, 1), (-1100000, 1000, 1)]\nInputs values: [array(110000), 'not shown', 'not shown', array(110000), 'not shown', 'not shown', 'not shown', array(100), 100, 'not shown', 'not shown', 'not shown', 'not shown', 'not shown']\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-139-491253632fcf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mjdx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjdx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mcompute_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mjdx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_space\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeature_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvoxel_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/tnaselar/anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    604\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthunks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m                         storage_map=self.fn.storage_map)\n\u001b[0m\u001b[0;32m    607\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m                     \u001b[1;31m# For the c linker We don't have access from\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/tnaselar/anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    593\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    596\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/tnaselar/anaconda/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[0;32m    670\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[0;32m    671\u001b[0m                  allow_gc=allow_gc):\n\u001b[1;32m--> 672\u001b[1;33m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    673\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/tnaselar/anaconda/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(node, args, outs)\u001b[0m\n\u001b[0;32m    659\u001b[0m                         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m                         \u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 661\u001b[1;33m                         self, node)\n\u001b[0m\u001b[0;32m    662\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mscan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform (/home/tnaselar/.theano/compiledir_Linux-3.13--generic-x86_64-with-debian-jessie-sid-x86_64-2.7.11-64/scan_perform/mod.cpp:3605)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mscan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform (/home/tnaselar/.theano/compiledir_Linux-3.13--generic-x86_64-with-debian-jessie-sid-x86_64-2.7.11-64/scan_perform/mod.cpp:3537)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Error allocating 220000000 bytes of device memory (out of memory).\nApply node that caused the error: GpuElemwise{mul,no_inplace}(GpuDimShuffle{0,x,1}.0, <CudaNdarrayType(float32, 3D)>)\nInputs types: [CudaNdarrayType(float32, (False, True, False)), CudaNdarrayType(float32, 3D)]\n\nHINT: Use another linker then the c linker to have the inputs shapes and strides printed.\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.\nApply node that caused the error: for{gpu,scan_fn}(Elemwise{minimum,no_inplace}.0, Subtensor{int64:int64:int8}.0, Subtensor{int64:int64:int8}.0, Elemwise{minimum,no_inplace}.0, GpuCAReduce{pre=sqr,red=add}{0,1,0}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, Elemwise{minimum,no_inplace}.0, ScalarFromTensor.0, GpuElemwise{sub,no_inplace}.0, GpuElemwise{sqr,no_inplace}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0)\nInputs types: [TensorType(int64, scalar), TensorType(int32, vector), TensorType(int32, vector), TensorType(int64, scalar), CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, 3D), TensorType(int64, scalar), Scalar(int64), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D)]\nInputs shapes: [(), (110000,), (110000,), (), (100, 500), (100, 500), (100, 1000, 500), (), (), (100, 1100, 500), (100, 1100, 500), (100, 1100, 500), (100, 1000, 500), (100, 1100, 1000)]\nInputs strides: [(), (4,), (4,), (), (500, 1), (500, 1), (500000, 500, 1), (), (), (550000, 500, 1), (550000, 500, 1), (-550000, 500, 1), (-500000, 500, 1), (-1100000, 1000, 1)]\nInputs values: [array(110000), 'not shown', 'not shown', array(110000), 'not shown', 'not shown', 'not shown', array(100), 100, 'not shown', 'not shown', 'not shown', 'not shown', 'not shown']\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "idx,jdx=np.mgrid[0:G,0:T].astype('int32')\n",
    "idx = idx.ravel()\n",
    "jdx = jdx.ravel()\n",
    "compute_grad(idx,jdx,model_space,feature_weights,voxel_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Tv = 200\n",
    "training_features\n",
    "training_signals\n",
    "validation_features\n",
    "validation_signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training loop with early stopping, mini-batch sgd\n",
    "http://deeplearning.net/tutorial/gettingstarted.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# early-stopping parameters\n",
    "patience = 5000  # look as this many examples regardless\n",
    "patience_increase = 2     # wait this much longer when a new best is\n",
    "                              # found\n",
    "improvement_threshold = 0.995  # a relative improvement of this much is\n",
    "                               # considered significant\n",
    "validation_frequency = min(n_train_batches, patience/2)\n",
    "                              # go through this many\n",
    "                              # minibatches before checking the network\n",
    "                              # on the validation set; in this case we\n",
    "                              # check every epoch\n",
    "\n",
    "best_params = None\n",
    "best_validation_loss = numpy.inf\n",
    "test_score = 0.\n",
    "start_time = time.clock()\n",
    "\n",
    "done_looping = False\n",
    "epoch = 0\n",
    "while (epoch < n_epochs) and (not done_looping):\n",
    "    # Report \"1\" for first epoch, \"n_epochs\" for last epoch\n",
    "    epoch = epoch + 1\n",
    "    for minibatch_index in xrange(n_train_batches):\n",
    "\n",
    "        d_loss_wrt_params = ... # compute gradient\n",
    "        params -= learning_rate * d_loss_wrt_params # gradient descent\n",
    "\n",
    "        # iteration number. We want it to start at 0.\n",
    "        iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "        # note that if we do `iter % validation_frequency` it will be\n",
    "        # true for iter = 0 which we do not want. We want it true for\n",
    "        # iter = validation_frequency - 1.\n",
    "        if (iter + 1) % validation_frequency == 0:\n",
    "\n",
    "            this_validation_loss = ... # compute zero-one loss on validation set\n",
    "\n",
    "            if this_validation_loss < best_validation_loss:\n",
    "\n",
    "                # improve patience if loss improvement is good enough\n",
    "                if this_validation_loss < best_validation_loss * improvement_threshold:\n",
    "\n",
    "                    patience = max(patience, iter * patience_increase)\n",
    "                best_params = copy.deepcopy(params)\n",
    "                best_validation_loss = this_validation_loss\n",
    "\n",
    "        if patience <= iter:\n",
    "            done_looping = True\n",
    "            break\n",
    "\n",
    "# POSTCONDITION:\n",
    "# best_params refers to the best out-of-sample parameters observed during the optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
